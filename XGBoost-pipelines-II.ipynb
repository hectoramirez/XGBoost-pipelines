{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost in a pipeline using the Cronic Kidney Disease dataset\n",
    "\n",
    "by Héctor Ramírez\n",
    "<hr>\n",
    "\n",
    "Throughout this example, we will be working with the Cronic Kidney Disease dataset from https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease. This dataset requires significantly more wrangling. \n",
    "\n",
    "The chronic kidney disease dataset contains both categorical and numeric features, but contains lots of missing values. The goal here is to predict who has chronic kidney disease given various blood indicators as features.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn_pandas import DataFrameMapper, CategoricalImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "import warnings; warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in each column:\n",
      " age                 9\n",
      "bp                 12\n",
      "sg                 47\n",
      "al                 46\n",
      "su                 49\n",
      "rbc               152\n",
      "pc                 65\n",
      "pcc                 4\n",
      "ba                  4\n",
      "bgr                44\n",
      "bu                 19\n",
      "sc                 17\n",
      "sod                87\n",
      "pot                88\n",
      "hemo               52\n",
      "pcv                71\n",
      "wc                106\n",
      "rc                131\n",
      "htn                 2\n",
      "dm                  2\n",
      "cad                 2\n",
      "appet               1\n",
      "pe                  1\n",
      "ane                 1\n",
      "classification      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('chronic_kidney_disease.csv', na_values='?')\n",
    "nulls_per_column = df.isnull().sum()\n",
    "print(\"Number of missing values in each column:\\n\", nulls_per_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Preprocessing data\n",
    "\n",
    "To use this dataset to train a model and use it into a pipeline, we need to do some preprocessing in advance. \n",
    "\n",
    "We will be using sklearn_pandas, which allows you to chain many more processing steps inside of a pipeline than are currently supported in scikit-learn. Specifically, we will impute missing categorical values directly using the Categorical_Imputer() class in sklearn_pandas, and the DataFrameMapper() class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "target = y.apply(lambda x: 0 if x=='ckd' else 1)\n",
    "\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "    [([numeric_feature], SimpleImputer(strategy=\"median\")) for numeric_feature in non_categorical_columns]\n",
    "    , input_df=True, df_out=True)\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(category_feature, CategoricalImputer()) for category_feature \n",
    "                                                in categorical_columns],\n",
    "                                                input_df=True, df_out=True)\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "We also create a function called Dictifier() to include into the pipeline to convert dataframes to dictionaries as needed for DictVectorizer():\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictifier(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if type(X) == pd.core.frame.DataFrame:\n",
    "            return X.to_dict(\"records\")\n",
    "        else:\n",
    "            return pd.DataFrame(X).to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Finally, the pipeline includes the FeatureUnion object made out of the mappers, the Dictifier(), the DictVectorizer() to get label- and onehot-encodings, and our XGBClassifier().\n",
    "<br><br>\n",
    "We perform a 3-fold cross_validation and compute the area under the receiver operating characteristic curve (AUC):\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-fold AUC:  0.998637406769937\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                     (\"featureunion\", numeric_categorical_union),\n",
    "                     (\"dictifier\", Dictifier()),\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "                     (\"clf\", xgb.XGBClassifier())\n",
    "                    ])\n",
    "\n",
    "cross_val_scores = cross_val_score(pipeline, X, target.values, scoring=\"roc_auc\", cv=3)\n",
    "\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Finally, let's perform a randomized search and identify the best hyperparameters:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9980266666666665\n",
      "\n",
      "The best set of parameters for this grid are:\n",
      " {'clf__n_estimators': 50, 'clf__max_depth': 9, 'clf__learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(0.05, 1, 0.05),\n",
    "    'clf__max_depth': np.arange(3, 10, 1),\n",
    "    'clf__n_estimators': np.arange(50, 200, 50)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline, param_distributions=gbm_param_grid, \n",
    "                                        cv=2, n_iter=2, scoring='roc_auc')\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_roc_auc.fit(X, target.values)\n",
    "\n",
    "# Compute metrics\n",
    "print(randomized_roc_auc.best_score_)\n",
    "print(\"\\nThe best set of parameters for this grid are:\\n\", randomized_roc_auc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
